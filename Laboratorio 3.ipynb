{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594877e2-6cfc-4614-a029-4b62acec1e6d",
   "metadata": {},
   "source": [
    "# Lab\t3\n",
    "# Procesando\tTexto\ty\tusando\tscikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b21d8-5e57-4565-bffd-be97d61ef38b",
   "metadata": {},
   "source": [
    "## 1 Procesamiento\tde\ttexto\tbásico\tcon\tSciKit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5ae42-8068-4c2b-a879-91ce640555d9",
   "metadata": {},
   "source": [
    "SciKit-learn\tes\tuna\tbiblioteca\tde\tPython\tde\tcódigo\tabierto\tpara\tel\taprendizaje\tde\t\n",
    "máquinas que\tviene\tcon\tinstalaciones\tbásicas\tpara\tel\tprocesamiento\tde\ttexto\tpara\t\n",
    "apoyar\tel\tagrupamiento\ty\tclasificación\t- incluyendo\ttokenización,\tconteo\tde\t\n",
    "palabras,\ty\tsteamming\t(obtener\tla\tforma\traíz\tde\tlas\tpalabras).\t\n",
    "En\teste lab\tpráctico\tvamos\ta\trevisar\tbrevemente\tcómo\tutilizar\tSciKit-learn\ten\t\n",
    "Python\t\ty\tluego\tobservar\tcon\tmás\tdetalle\ten\tlas\tinstalaciones\tde\tprocesamiento\t\n",
    "previo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670458da-4949-4e69-b909-b3722a9aeb61",
   "metadata": {},
   "source": [
    "## 2.1\tEl\tuso\tde\tSciKit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86681ac-3ff2-4f53-b0d2-be0d48974df2",
   "metadata": {},
   "source": [
    "Esta\tsección\tpuede\tser\tomitido\tpor\taquellos\tde\tustedes\tque\tya\testán\tfamiliarizados\tcon\t\n",
    "la\tbiblioteca]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdbf69f-9f4b-4adb-a42b-58fdb1bf0898",
   "metadata": {},
   "source": [
    "## 3.\tPre-procesamiento\tmás\tavanzado\tcon\tNLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206babbc-6ea8-43b2-bdba-4112f1a1eabf",
   "metadata": {},
   "source": [
    "NLTK\tes\tuna\tbiblioteca\tde\tPython\tde\tcódigo\tabierto que\tya\tusamos\ten\tlos\tLabs\t anteriores disponible\ten:\n",
    "http://www.nltk.org\n",
    "NLKT\tes\tcompatible\tcon\tla\tmayoría\tde\tlos\ttipos\tde\tprocesamiento previo\n",
    ",\tdesde\tPOS\ttagging\tpara\tfragmentar,\tpara Inglés. También\tviene\tcon\t\n",
    "varios\trecursos\tútiles\tcomo\tcorpus\ty\tel\tléxico.\n",
    "NLTK\tse\tdescribe\ten\tdetalle\ten\tun\tlibro\tde\tBird,\tKlein\ty\tLoper\tdisponible\ten\tlínea:\n",
    "http://www.nltk.org/book/ la\tversión\tpara\tPython\t3\n",
    "NLTK\tes\tuna\tbiblioteca\tenorme,\tmucho\tmás\tgrande\tque\tSciKit-learn\tde\thecho,\t\n",
    "ya\tque\tcontiene\ttambién\tsu\tpropia\timplementación\tde\tmuchos\talgoritmos\tde\t\n",
    "aprendizaje\tautomático.\t\n",
    "Cómo\taclaramos\ten\tclase,\tsólo\tvamos\ta\ttratar\tacá\ten\teste\tcurso\tde\tsus\t\n",
    "funcionalidades.\n",
    "\n",
    ">>> import nltk\n",
    "\n",
    "El\tsteamming\ten\t NLTK incluye\timplementaciones\tde\tvarios algoritmos\n",
    "muy\tconocidos y\tutilizados, incluyendo\tel\tPorter\tStemmer\ty\tel\tLancaster\n",
    "Stemmer.\t(Ver http://www.nltk.org/howto/stem.html para\tuna\t\n",
    "introducción\tgeneral\ty http://www.nltk.org/api/nltk.stem.html\t\tpara\t\n",
    "más\tdetalles,\tincluyendo\tlos\tidiomas\tcubiertos)\t\n",
    "Para\tcrear\tun steammer\tde\tInglés\tque\ttiene\tque\thacer\tlo siguiente:\n",
    ">>> s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b188d1b-cca8-4e05-80b5-6807f02f8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77f22fe-f938-475b-baa4-b3b1b206abd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x7ffb02f425c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db75af-4069-4fb7-a4ab-268ce39f52fd",
   "metadata": {},
   "source": [
    "Después\tde\tcrear\tel\tsteammer,\ta\tcontinuación,\tpuede\tutilizarlo\tpara\tllevar\ta\tla\t\n",
    "raíz\t(steam) palabras\tde\tla\tsiguiente\tmanera:\n",
    ">>> s.stem(\"cats\")\n",
    "\n",
    "u'cat'\n",
    ">>> s.stem(\"loving\")\n",
    "\n",
    "Otros\ttipos\tde\tpre-procesamiento\tde\tNLTK\tincluye\timplementaciones\tde\t\n",
    "muchos\tde\tlos\tmódulos\tde\tprocesamiento previo\ty\tanalizadores\t\n",
    "sintácticos\tque\tdiscutimos\to\tdiscutiremos\ten\tlas clases:\n",
    "    - identificadores\tde\tidioma\n",
    "    - tokenizers\tpara\tvarios\tidiomas\n",
    "    - divisores\tde\toraciones\n",
    "    - POS\ttaggers\n",
    "    - Chunkers\n",
    "    - Parsers\t\n",
    "Además,\tNLTK\tincluye\timplementaciones\tde\tlos\taspectos\tdel\tanálisis\tde\ttexto\t\n",
    "que\tvamos\ta\tdiscutir\ten\teste\tmódulo,\tincluyendo\n",
    "    - NER\t(Named\tEntity\tRecognition)\n",
    "    - Análisis\tde\tlos\tsentimientos\n",
    "    - Extraer\tinformación\tde\tlos\tmedios\tde\tredes sociales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2ee77cd-52b0-46d3-92d0-42125d4ba98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6f0e0b-5767-4ecc-ac47-f17be8fb3c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"loving\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3bc698-dec4-4238-9fa1-af3cb5d7c497",
   "metadata": {},
   "source": [
    "Por\tejemplo,\tlas\tinstrucciones\tsiguientes\t(\tpuede\tque\ttengas que\tdescargar\tel\tpaquete\t NLTK\t'punkt'\tpara\thacer\testo)\n",
    ">>> from nltk.tokenize import word_tokenize\n",
    ">>> nltk.download(‘punkt’) #if not used nltk.download()\n",
    ">>> text = word_tokenize(\"And now for something completely different\")\n",
    "\n",
    "producir\tuna\tversión\ttokenizada\tde\tla\tfrase,\tque\tluego\tpuede\tser\talimentado\ten\tel\t\n",
    "etiquetador\tPOS\t(\tpuede\tque tenga\tque\tdescargar\tel\tpaquete\t'\tmaxent_...'\tpara\thacer\t\n",
    "esto)\n",
    "\n",
    ">>> nltk.download('averaged_perceptron_tagger')\n",
    ">>> nltk.download('maxent_treebank_pos_tagger')\n",
    ">>> nltk.pos_tag(text)\n",
    "\n",
    "    [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something',\n",
    "    'NN'), ('completely', 'RB'), ('different', 'JJ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e87556-a44f-4673-8cf4-d12280a84d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/desalvo_fabiano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\") \n",
    "text = word_tokenize(\"And now for something completely different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e751437-bea2-46bf-b9c4-460b739af922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And', 'now', 'for', 'something', 'completely', 'different']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "680f3abb-815c-4c2f-b72d-0ad489f94960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/desalvo_fabiano/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     /home/desalvo_fabiano/nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80307d9c-31cd-49d7-9aaf-472307cbbd5d",
   "metadata": {},
   "source": [
    "# 4.\tLa\tintegración\tel\tsteammer\tde\tNLTK\tcon\tel\tCountVectorizer\tde\tSciKit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff4f19-e4be-47c2-922c-d9600876a784",
   "metadata": {},
   "source": [
    "El\tsteammer\tde\tNLTK\tpuede\tser\tutilizado\tantes\tde\tla\talimentación\ten\tCountVectorizer\t\n",
    "de SciKit-learn,\tobteniendo\tasí\tun\tíndice\tmás\tcompacto.\t\n",
    "Una\tforma\tde\thacer\testo\tes\tdefinir\tuna\tnueva\tclase\tStemmedCountVectorizer\n",
    "Extendiendo\tde CountVectorizer\ty\tredefiniendo\tel\tmétodo\tbuild_analyzer\t()\n",
    "que\tse\tencarga\tde\tpre-procesamiento\ty\ttokenización:\n",
    "http://scikit- learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "build_analyzer\t()\ttoma\tun\tstring\tcomo\tentrada\ty\tcomo\tsalida\tuna\tlista\tde\ttokens:\n",
    ">>> from sklearn.feature_extraction.text import CountVectorizer\n",
    ">>> vectorizer = CountVectorizer(stop_words='english')\n",
    ">>> analyze = vectorizer.build_analyzer()\n",
    ">>> analyze(\"John bought carrots and potatoes\")\n",
    "[u'john', u'bought', u'carrots', u'potatoes']\n",
    "\n",
    "Si\t modificamos\tbuild_analyzer\t()\tpara\taplicar\t\tel\tsteammer\tde\tNLTK\ta\tla\tsalida\tdel\t\n",
    "método\tbuild_analyzer\t(),\tobtenemos\tuna\tversión\tque\tderiva\tasí:\n",
    ">>>\timport\tnltk.stem\n",
    ">>>\tenglish_stemmer\t=\tnltk.stem.SnowballStemmer(‘english’)\t\n",
    ">>>\tclass\tStemmedCountVectorizer(CountVectorizer):\n",
    "\n",
    "def\tbuild_analyzer(self):\n",
    "analyzer\t=\tsuper(StemmedCountVectorizer,\tself).build_analyzer()\n",
    "return\tlambda\tdoc:\t(english_stemmer.stem(w)\tfor\tw\tin\tanalyzer(doc))\t\n",
    "Ahora\tPodemos\tcrear\tuna\tinstancia\tde\tnuestra\tclase!\t\n",
    ">>> stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    ">>> stem_analyze = stem_vectorizer.build_analyzer()\n",
    "\n",
    "como\tse\tpuede\tver,\testos\tnuevos\tusos\tVectorizer\tsurgieron\tversiones\tde\tfichas:\n",
    ">>> Y = stem_analyze(\"John bought carrots and potatoes\")\n",
    ">>> for tok in Y:\n",
    ">>> print(tok)\n",
    "\n",
    "john\n",
    "bought\n",
    "carrot\n",
    "potato\n",
    "Si\tutilizamos\teste\tVectorizer\tpara\textraer\tfeatures\tpara\tel\tsubconjunto\tdel\t\n",
    "dataset\t20_Newsgroups que\tconsideramos antes,\tvamos\ta\ttener\tun\tmenor\t\n",
    "número\tde\tfeatures:\n",
    ">>> from sklearn.datasets import fetch_20newsgroups\n",
    ">>> categories = ['alt.atheism','soc.religion.christian','comp.graphics', 'sci.med']\n",
    ">>> twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True,\n",
    "random_state=42)\n",
    ">>> train_counts = stem_vectorizer.fit_transform(twenty_train.data) >>>\n",
    "len(stem_vectorizer.get_feature_names())\n",
    "26889\n",
    "(Compará\teste\tnúmero\tcon\tlos\talrededor\tde\t35.000\tcaracterísticas\tque\themos\tobtenido\t\n",
    "usando la versión sin hacer steam)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839405f7-4303-481d-a15a-fdbf8efa73d9",
   "metadata": {},
   "source": [
    "Subir a tu github una implementación personalizada de NLTK para CountVectorizer\n",
    "que haga steam y stopwords del idioma español y dos ejemplos de oraciones usando tu clase.\n",
    "También importá un corpus como 20_Newsdataset pero que esté en español. Qué\n",
    "corpus poner, queda a tu criterio! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f8940b-a81d-4ef1-8467-285276f693f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
     ]
    }
   ],
   "source": [
    "#Exploramos los lenguajes soportados\n",
    "from nltk.stem import SnowballStemmer\n",
    "print(\" \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed6bde9-06e5-4927-bc8b-54e30f15fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Si\t modificamos\tbuild_analyzer\t()\tpara\taplicar\t\tel\tsteammer\tde\tNLTK\ta\tla\tsalida\tdel\t\n",
    "#método\tbuild_analyzer\t(),\tobtenemos\tuna\tversión\tque\tderiva\tasí:\n",
    "import\tnltk.stem\n",
    "spanish_stemmer\t=\tnltk.stem.SnowballStemmer('spanish')\t\n",
    "spanish_stop_words = stopwords.words('spanish')\n",
    "\n",
    "class\tStemmedCountVectorizer(CountVectorizer):\n",
    "    def\tbuild_analyzer(self):\n",
    "        analyzer\t=\tsuper(StemmedCountVectorizer,\tself).build_analyzer()\n",
    "        return\tlambda\tdoc:\t(spanish_stemmer.stem(w)\tfor\tw\tin\tanalyzer(doc))\t\n",
    "#Ahora\tPodemos\tcrear\tuna\tinstancia\tde\tnuestra\tclase!\t\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df=1, stop_words=spanish_stop_words)\n",
    "stem_analyze_spanish = stem_vectorizer.build_analyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b4bca69-f083-40a8-990c-5a659df2b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intent\n",
      "impos\n",
      "realiz\n",
      "posibl\n",
      "\n",
      "\n",
      "si\n",
      "ric\n",
      "perplej\n",
      "certez\n"
     ]
    }
   ],
   "source": [
    "#dos ejemplos de oraciones usando tu clase.\n",
    "\n",
    "Y = stem_analyze_spanish(\"Es intentando lo imposible como se realiza lo posible.\")\n",
    "for tok in Y:\n",
    "    print(tok)\n",
    "\n",
    "print(\"\\n\")\n",
    "    \n",
    "Z = stem_analyze_spanish(\"Si de algo soy rico es de perplejidades y no de certezas\")\n",
    "for tok in Z:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff525aa-424c-4790-a626-145015de0db8",
   "metadata": {},
   "source": [
    "Importación de corpus en español \n",
    "de https://archive.ics.uci.edu/ml/machine-learning-databases/00410/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb80ff80-21f9-4718-af61-94fe77cd13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e817758b-cea8-4795-a762-adb2e5608bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'id': 1, 'preliminary_decision': 'accept', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'id': 2, 'preliminary_decision': 'accept', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'id': 3, 'preliminary_decision': 'accept', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'id': 4, 'preliminary_decision': 'accept', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'id': 5, 'preliminary_decision': 'accept', 'r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               paper\n",
       "0  {'id': 1, 'preliminary_decision': 'accept', 'r...\n",
       "1  {'id': 2, 'preliminary_decision': 'accept', 'r...\n",
       "2  {'id': 3, 'preliminary_decision': 'accept', 'r...\n",
       "3  {'id': 4, 'preliminary_decision': 'accept', 'r...\n",
       "4  {'id': 5, 'preliminary_decision': 'accept', 'r..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15781f18-55b0-4317-8d7b-46348ae37489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO tratamiento del dataset en caso de que sirva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
